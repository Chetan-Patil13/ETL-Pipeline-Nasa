# ğŸš€ Airflow ETL Pipeline with Postgres and API Integration

## ğŸ“Œ Project Overview

This project involves building an ETL (Extract, Transform, Load) pipeline using **Apache Airflow**, **PostgreSQL**, and **NASA's Astronomy Picture of the Day (APOD) API**. The pipeline is designed to extract data from the external API, transform it into a structured format, and load it into a **Postgres database**.

Airflow acts as the orchestrator, managing task dependencies and scheduling the entire workflow. **Docker** is used to containerize Airflow and Postgres services, ensuring an isolated and reproducible environment.

---

## ğŸ”— Key Components

### ğŸ—ï¸ **Airflow for Orchestration**

- Defines, schedules, and monitors the ETL pipeline.
- Uses a **Directed Acyclic Graph (DAG)** to structure task dependencies.
- Handles task execution reliability and monitoring.

### ğŸ—„ï¸ **PostgreSQL Database**

- Stores the extracted and transformed data.
- Runs as a **Docker container**, ensuring easy management and persistent data storage.
- Utilizes **PostgresHook** and **PostgresOperator** in Airflow for interaction.

### ğŸŒŒ **NASA API (Astronomy Picture of the Day)**

- External API providing daily astronomy-related images and metadata.
- Includes fields such as `title`, `explanation`, and `image URL`.
- Accessed via **Airflow's SimpleHttpOperator** to extract data.

---

## ğŸ¯ Project Objectives

âœ” **Extract Data**:

- Fetch astronomy-related metadata daily from NASA's **APOD API**.

âœ” **Transform Data**:

- Process and filter the API response to ensure a structured format.

âœ” **Load Data**:

- Store the processed data in a **PostgreSQL database** for further analysis and reporting.

---

## ğŸ›ï¸ Architecture & Workflow

The ETL pipeline follows the **DAG (Directed Acyclic Graph)** structure in Airflow, with the following key stages:

1ï¸âƒ£ **Extract (E):**

- Uses `SimpleHttpOperator` to send `GET` requests to NASA's APOD API.
- Receives JSON responses containing image metadata.

2ï¸âƒ£ **Transform (T):**

- Processes and extracts relevant fields (`title`, `explanation`, `url`, `date`).
- Ensures data is formatted correctly before loading into Postgres.
- Utilizes Airflowâ€™s **TaskFlow API** (`@task` decorator) for transformation.

3ï¸âƒ£ **Load (L):**

- Uses `PostgresHook` to insert structured data into a **Postgres table**.
- Creates the table automatically if it doesn't exist.

---

## ğŸ› ï¸ Tech Stack

- **Apache Airflow** ğŸŒ€ â€“ Workflow orchestration
- **PostgreSQL** ğŸ—ƒï¸ â€“ Database storage
- **Docker** ğŸ³ â€“ Containerization
- **Python** ğŸ â€“ Scripting and data processing
- **NASA APOD API** ğŸŒ  â€“ External data source

---

## ğŸ Getting Started

### ğŸ”¹ Prerequisites

Ensure you have the following installed:

- **Docker & Docker Compose**
- **Python 3.8+**
- **Git**

### ğŸ”¹ Installation & Setup

1. **Clone the Repository**:
   ```sh
   git clone https://github.com/Chetan-Patil13/ETL-Pipeline-Nasa.git
   cd ETL-Pipeline-Nasa
   ```
2. **Start the Airflow & Postgres Services**:
   ```sh
   docker-compose up -d
   ```
3. **Access Airflow UI**:

   - Open [http://localhost:8080](http://localhost:8080)
   - Default credentials: `airflow/airflow`

4. **Trigger the DAG**:
   - Navigate to `ml_pipeline` in the **Airflow UI**
   - Click **Trigger DAG** â–¶ï¸

---

## ğŸ“Š Expected Output

- Data from NASA's APOD API gets extracted and stored in **Postgres**.
- The pipeline runs **daily** as per the schedule defined in Airflow.
- Logs and task status can be monitored in **Airflow UI**.

---

## ğŸ“Œ Future Improvements

ğŸ”¹ Add **data validation** before inserting into Postgres.
ğŸ”¹ Implement **logging & alerting** for failures.
ğŸ”¹ Integrate **visualization tools** like Power BI or Tableau.

---

## ğŸ™Œ Contribution

Feel free to fork this repository and contribute! Suggestions and pull requests are welcome. ğŸ˜Š

---

## ğŸ“œ License

This project is licensed under the **MIT License**.

---

ğŸš€ **Happy Coding & Keep Exploring the Universe! ğŸŒŒ**
